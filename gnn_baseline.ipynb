{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c97ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2753b61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    print(f\"Loaded embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "except ImportError:\n",
    "    embedder = None\n",
    "    print(\"sentence-transformers not installed. Please install it to embed text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17da4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "796d02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_author_embedding(abstracts):\n",
    "    if not embedder or len(abstracts) == 0:\n",
    "        return torch.zeros(384, dtype=torch.float)  # 384 is dimension of all-MiniLM-L6-v2\n",
    "    abs_embeddings = embedder.encode(abstracts, convert_to_tensor=True)\n",
    "    return abs_embeddings.mean(dim=0)  # shape: [embedding_dim]\n",
    "\n",
    "def compute_topic_embedding(node_id, topic_name=\"\"):\n",
    "    if not embedder:\n",
    "        return torch.zeros(384, dtype=torch.float)\n",
    "    text_to_embed = node_id\n",
    "    \n",
    "    topic_emb = embedder.encode([text_to_embed], convert_to_tensor=True)\n",
    "    return topic_emb[0]  # shape: [embedding_dim]\n",
    "\n",
    "\n",
    "def add_node_id_attribute(G):\n",
    "    # If G has N nodes and each node is labeled by something (e.g. \"A123\" or 123),\n",
    "    # we store that label in the 'node_id' attribute.\n",
    "    nx.set_node_attributes(\n",
    "        G,\n",
    "        {node: str(node) for node in G.nodes()},\n",
    "        name=\"node_id\"\n",
    "    )\n",
    "    return G\n",
    "\n",
    "\n",
    "def convert_graph_to_pyg(G):\n",
    "    # Ensure node_id is set\n",
    "    G = add_node_id_attribute(G)\n",
    "\n",
    "    # Now from_networkx will pick up the node_id attribute\n",
    "    pyg_data = from_networkx(G)\n",
    "\n",
    "    return pyg_data\n",
    "\n",
    "\n",
    "def load_and_unify_attributes(graph_pickle_path=\"niu_graph.pkl\"):\n",
    "    # Load the NetworkX graph from a pickle and ensure every node has the same set of attributes\n",
    "    with open(graph_pickle_path, \"rb\") as f:\n",
    "        G = pickle.load(f)\n",
    "\n",
    "    for node_id, data in G.nodes(data=True):\n",
    "        # node_type\n",
    "        data.setdefault(\"node_type\", \"unknown\")\n",
    "\n",
    "        # If it's an author, fill in missing fields with sensible defaults:\n",
    "        if data[\"node_type\"] == \"author\":\n",
    "            data.setdefault(\"works_count\", 0.0)\n",
    "            data.setdefault(\"cited_by_count\", 0.0)\n",
    "            data.setdefault(\"abstracts\", [])\n",
    "        elif data[\"node_type\"] == \"topic\":\n",
    "            # For topics, we won't have numeric or abstract data\n",
    "            data.setdefault(\"works_count\", 0.0)\n",
    "            data.setdefault(\"cited_by_count\", 0.0)\n",
    "            data.setdefault(\"abstracts\", [])\n",
    "        else:\n",
    "            # fallback for unknown node types\n",
    "            data.setdefault(\"works_count\", 0.0)\n",
    "            data.setdefault(\"cited_by_count\", 0.0)\n",
    "            data.setdefault(\"abstracts\", [])\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def build_pyg_data(G):\n",
    "    print(\"Embedding node information...\")\n",
    "\n",
    "    # Embed each node and store in a dict\n",
    "    node_embeddings = {}\n",
    "    embedding_dim = 384  # dimension for all-MiniLM-L6-v2; change if using a different model\n",
    "\n",
    "    for node_id, attrs in G.nodes(data=True):\n",
    "        node_type = attrs.get(\"node_type\", \"unknown\")\n",
    "\n",
    "        if node_type == \"author\":\n",
    "            # numeric features\n",
    "            wc = float(attrs.get(\"works_count\", 0))\n",
    "            cc = float(attrs.get(\"cited_by_count\", 0))\n",
    "\n",
    "            # text embedding (from abstracts)\n",
    "            abstracts = attrs.get(\"abstracts\", [])\n",
    "            author_emb = compute_author_embedding(abstracts)\n",
    "\n",
    "            # combine numeric + text (2 + embedding_dim)\n",
    "            numeric_vec = torch.tensor([wc, cc], dtype=torch.float)\n",
    "            full_vec = torch.cat([numeric_vec, author_emb], dim=0)\n",
    "\n",
    "        elif node_type == \"topic\":\n",
    "            # numeric features: [0,0]\n",
    "            numeric_vec = torch.zeros(2, dtype=torch.float)\n",
    "\n",
    "            # text embedding (from node_id)\n",
    "            topic_emb = compute_topic_embedding(node_id)\n",
    "\n",
    "            full_vec = torch.cat([numeric_vec, topic_emb], dim=0)\n",
    "        else:\n",
    "            # fallback for unknown node types\n",
    "            # just zero everything\n",
    "            numeric_vec = torch.zeros(2, dtype=torch.float)\n",
    "            zero_emb = torch.zeros(embedding_dim, dtype=torch.float)\n",
    "            full_vec = torch.cat([numeric_vec, zero_emb], dim=0)\n",
    "\n",
    "        node_embeddings[node_id] = full_vec\n",
    "\n",
    "    # Now convert to PyG\n",
    "    pyg_data = from_networkx(G)\n",
    "\n",
    "    # Build final feature matrix pyg_data.x\n",
    "    num_nodes = pyg_data.num_nodes\n",
    "    all_features = []\n",
    "\n",
    "    # from_networkx typically keeps a 'node' attribute array with the original node_id\n",
    "    for i in range(num_nodes):\n",
    "        node_id = pyg_data[\"node_id\"][i]  # the original ID\n",
    "        # Retrieve the precomputed vector\n",
    "        feat_vec = node_embeddings[node_id]\n",
    "        all_features.append(feat_vec)\n",
    "\n",
    "    x = torch.stack(all_features, dim=0)\n",
    "    pyg_data.x = x\n",
    "\n",
    "    return pyg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0da48985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x  # shape [num_nodes, out_channels]\n",
    "\n",
    "    def predict_link(self, emb, edge_pairs):\n",
    "        \"\"\"\n",
    "        Dot-product similarity -> sigmoid as probability of link existence.\n",
    "        \"\"\"\n",
    "        u = edge_pairs[0]\n",
    "        v = edge_pairs[1]\n",
    "        score = (emb[u] * emb[v]).sum(dim=-1)\n",
    "        return torch.sigmoid(score)\n",
    "\n",
    "\n",
    "def train_test_link_prediction(pyg_data, epochs=20, lr=0.01, hidden_dim=32):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # A) Identify authors vs. topics from node_type\n",
    "    #    If node_type is a Python list, we can do:\n",
    "    node_types = list(pyg_data[\"node_type\"])  # e.g. 100-length list of b'author' or b'topic'\n",
    "    # Convert bytes to str if needed\n",
    "    node_types = [nt.decode(\"utf-8\") if isinstance(nt, bytes) else nt for nt in node_types]\n",
    "\n",
    "    # We'll gather indices of authors\n",
    "    author_indices_list = [i for i, nt in enumerate(node_types) if nt == \"author\"]\n",
    "    author_indices_torch = torch.tensor(author_indices_list, dtype=torch.long)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # B) Filter edges for co_author (author-author)\n",
    "    edge_index = pyg_data.edge_index\n",
    "    relations = pyg_data[\"relation\"]  # likely a list of b'co_author', b'has_topic', etc.\n",
    "\n",
    "    # Convert relations to a list of strings\n",
    "    if isinstance(relations, torch.Tensor):\n",
    "        # Might be a tensor of bytes or python objects. Let's turn them into normal Python strings\n",
    "        relations = [r.decode(\"utf-8\") if isinstance(r, bytes) else str(r) for r in relations.tolist()]\n",
    "    else:\n",
    "        # if it's already a list, just ensure they're strings\n",
    "        relations = [r.decode(\"utf-8\") if isinstance(r, bytes) else str(r) for r in relations]\n",
    "\n",
    "    coauthor_edges = []\n",
    "    for i, rel in enumerate(relations):\n",
    "        if rel == \"co_author\":\n",
    "            src = edge_index[0, i].item()\n",
    "            dst = edge_index[1, i].item()\n",
    "            # check that both are authors\n",
    "            if node_types[src] == \"author\" and node_types[dst] == \"author\":\n",
    "                coauthor_edges.append((src, dst))\n",
    "\n",
    "    if len(coauthor_edges) < 2:\n",
    "        print(\"Not enough co_author edges for training.\")\n",
    "        return None, None, None\n",
    "\n",
    "    pos_edge_count = len(coauthor_edges)\n",
    "    # Turn into a torch tensor of shape [2, n]\n",
    "    coauthor_edges_t = torch.tensor(coauthor_edges, dtype=torch.long).t()\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # C) Negative sampling: random author-author pairs not in co_author\n",
    "    existing_pairs = set()\n",
    "    for a, b in coauthor_edges:\n",
    "        if a > b:\n",
    "            a, b = b, a\n",
    "        existing_pairs.add((a, b))\n",
    "\n",
    "    needed_neg_count = pos_edge_count\n",
    "    author_list = author_indices_list\n",
    "    negative_edges_list = []\n",
    "    attempts = 0\n",
    "    max_attempts = needed_neg_count * 10\n",
    "\n",
    "    while len(negative_edges_list) < needed_neg_count and attempts < max_attempts:\n",
    "        a = random.choice(author_list)\n",
    "        b = random.choice(author_list)\n",
    "        if a == b:\n",
    "            attempts += 1\n",
    "            continue\n",
    "        if a > b:\n",
    "            a, b = b, a\n",
    "        if (a, b) not in existing_pairs:\n",
    "            negative_edges_list.append((a, b))\n",
    "        attempts += 1\n",
    "\n",
    "    if len(negative_edges_list) < needed_neg_count:\n",
    "        needed_neg_count = len(negative_edges_list)\n",
    "\n",
    "    neg_edges_t = torch.tensor(negative_edges_list[:needed_neg_count], dtype=torch.long).t()\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # D) Train/test split\n",
    "    num_pos = coauthor_edges_t.size(1)\n",
    "    perm_pos = torch.randperm(num_pos)\n",
    "    split_pos = int(num_pos * 0.8)\n",
    "    train_pos = coauthor_edges_t[:, perm_pos[:split_pos]]\n",
    "    test_pos = coauthor_edges_t[:, perm_pos[split_pos:]]\n",
    "\n",
    "    num_neg = neg_edges_t.size(1)\n",
    "    perm_neg = torch.randperm(num_neg)\n",
    "    split_neg = int(num_neg * 0.8)\n",
    "    train_neg = neg_edges_t[:, perm_neg[:split_neg]]\n",
    "    test_neg = neg_edges_t[:, perm_neg[split_neg:]]\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # E) Build & train model\n",
    "    # We'll do a trivial x=pyg_data.x if you have numeric/embedding features\n",
    "    # or we can do x=some dummy if you haven't built your features yet.\n",
    "    x = pyg_data.x\n",
    "    if x is None:\n",
    "        print(\"Warning: No features found in pyg_data.x. Using a random feature vector.\")\n",
    "        x = torch.rand(pyg_data.num_nodes, 8)  # random fallback\n",
    "\n",
    "    in_channels = x.size(1)\n",
    "    model = GraphSAGEModel(in_channels, hidden_dim, out_channels=64).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    x = x.to(device)\n",
    "    full_edge_index = pyg_data.edge_index.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        emb = model(x, full_edge_index)\n",
    "        pos_score = model.predict_link(emb, train_pos.to(device))\n",
    "        neg_score = model.predict_link(emb, train_neg.to(device))\n",
    "\n",
    "        pos_labels = torch.ones(pos_score.size(0), device=device)\n",
    "        neg_labels = torch.zeros(neg_score.size(0), device=device)\n",
    "\n",
    "        all_scores = torch.cat([pos_score, neg_score], dim=0)\n",
    "        all_labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
    "\n",
    "        loss = F.binary_cross_entropy(all_scores, all_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # F) Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb = model(x, full_edge_index)\n",
    "        test_pos_score = model.predict_link(emb, test_pos.to(device))\n",
    "        test_neg_score = model.predict_link(emb, test_neg.to(device))\n",
    "\n",
    "        # combine\n",
    "        test_scores = torch.cat([test_pos_score, test_neg_score], dim=0).cpu().numpy()\n",
    "        test_labels = torch.cat([\n",
    "            torch.ones(test_pos_score.size(0)),\n",
    "            torch.zeros(test_neg_score.size(0))\n",
    "        ]).cpu().numpy()\n",
    "\n",
    "        # AUC\n",
    "        auc = roc_auc_score(test_labels, test_scores)\n",
    "\n",
    "        # Precision & Recall at threshold 0.5\n",
    "        binary_preds = (test_scores >= 0.5).astype(int)\n",
    "        precision = precision_score(test_labels, binary_preds)\n",
    "        recall = recall_score(test_labels, binary_preds)\n",
    "\n",
    "        print(f\"\\nTest AUC: {auc:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "    return model, auc, (precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a4fec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph from niu_graph.pkl...\n"
     ]
    }
   ],
   "source": [
    "# Load pickled NetworkX graph\n",
    "print(\"Loading graph from niu_graph.pkl...\")\n",
    "G = load_and_unify_attributes(\"niu_graph.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98b713a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build PyG data, embedding abstracts & topic IDs\n",
    "pyg_data = convert_graph_to_pyg(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae7e06ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GNN for co-author link prediction (with abstract & topic embeddings)...\n",
      "Warning: No features found in pyg_data.x. Using a random feature vector.\n",
      "Epoch 5/20, Loss = 0.7751\n",
      "Epoch 10/20, Loss = 0.7070\n",
      "Epoch 15/20, Loss = 0.6999\n",
      "Epoch 20/20, Loss = 0.6978\n",
      "\n",
      "Test AUC: 0.5666\n",
      "Precision: 0.5000, Recall: 1.0000\n",
      "\n",
      "Done with training!\n",
      "Final metrics: AUC=0.5666, Precision=0.5000, Recall=1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train GNN for link prediction\n",
    "print(\"Training GNN for co-author link prediction (with abstract & topic embeddings)...\")\n",
    "model, auc, (precision, recall) = train_test_link_prediction(\n",
    "    pyg_data,\n",
    "    epochs=20,\n",
    "    lr=0.01,\n",
    "    hidden_dim=32\n",
    ")\n",
    "if model:\n",
    "    print(\"\\nDone with training!\")\n",
    "    print(f\"Final metrics: AUC={auc:.4f}, Precision={precision:.4f}, Recall={recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b056a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
